{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be the exmerimentation file for creating categories from the initial data pull\n",
    "\n",
    "# I was going off of this website for inspiration https://towardsdev.com/mastering-data-clustering-with-embedding-models-87a228d67405\n",
    "\n",
    "# Step 0: Clean and tokenize the data, doing lowered depunctuated title + category ids separated by a single whitespace\n",
    "# Step 1: Make semantic text embeddings from the item headings probably categories,\n",
    "# Step 2: Use these embeddings to do some kind of clustering algorithm and have these be properly clustered\n",
    "# Step 3: Have a llm go through these clusters of data and create meaningful category suggestions by feeding in each of \n",
    "# the clustered data through and suggest a category\n",
    "\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install --upgrade torch\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ast\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47304f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with db libarary\n",
    "\n",
    "conn = sqlite3.connect(\"./databases/Playstation 5.db\")\n",
    "sql = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "print(conn.execute(sql).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Pull all of the data that is necessaary out and preprocess it into tokens for the embedding process\n",
    "\n",
    "db_paths = []\n",
    "db_path_str = \"./databases\"\n",
    "for db in os.listdir(db_path_str):\n",
    "    db_paths.append(os.path.join(db_path_str, db))\n",
    "\n",
    "db_connections = [sqlite3.connect(conn) for conn in db_paths]\n",
    "data = {} # Data in a dictnioary with {table : all items}\n",
    "items = [] # All of the tables in each of the databases to look at, used as key to the data dict\n",
    "for db_connection in db_connections:\n",
    "    tables_sql = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "\n",
    "    tables = db_connection.execute(tables_sql).fetchall()\n",
    "    main_table = tables[0][0]\n",
    "    print(main_table)\n",
    "    items.append(main_table)\n",
    "\n",
    "    select_all_sql = f\"SELECT title, categories FROM '{main_table}'\"\n",
    "    item_data = db_connection.execute(select_all_sql).fetchall()\n",
    "\n",
    "    for item in item_data:\n",
    "        data[main_table] = item_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting and Cleaning the data\n",
    "\n",
    "\n",
    "formatted_item_list = []\n",
    "# Saving all of the unncecessary stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "for item in data[\"Playstation 5\"]:\n",
    "    # extracting the item category ids\n",
    "    category_ids = []\n",
    "    title = item[0]\n",
    "\n",
    "    for category in ast.literal_eval(item[1]):\n",
    "        category_ids.append(category[\"categoryId\"])\n",
    "        \n",
    "    # Putting together the category ids and title\n",
    "    formatted_item = \" \".join([title, \" \".join(category_ids)])\n",
    "\n",
    "    # Joining together the lowered string \n",
    "    formatted_item = formatted_item.lower()\n",
    "\n",
    "    # Removing all punctuation\n",
    "    formatted_item = re.sub(r'[^\\w\\s]', '', formatted_item)\n",
    "\n",
    "    # Sometimes the thing removes something that looks like this and leaves 2 spaces instead of 1\n",
    "    # Going to make it to where there are no '' in the tokens\n",
    "    formatted_item = \" \".join([x for x in formatted_item.split(\" \") if x != ''])\n",
    "\n",
    "    # Removing all stop words and joining to final string\n",
    "    formatted_item_list.append(\" \".join([x for x in formatted_item.split(\" \") if x not in stop_words]))\n",
    "    print(formatted_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efacbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to loginto huggingface to use the model\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "with open(\"huggingface_credentials.json\", \"r\") as f:\n",
    "    t = json.load(f)[\"token\"]\n",
    "login(token=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33912d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the playstation data\n",
    "# Aparently the tokenizer is not necessary for this library\n",
    "# the embedding model will take in raw text and do the tokenization itself\n",
    "# Length of the numpy array embeddings for this model is 768\n",
    "\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "embeddings = []\n",
    "for item in formatted_item_list:\n",
    "    \n",
    "    embeddings.append(model.encode_query(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to do the kmeans clustering on the data now\n",
    "# Going to try out the elbow method and see how many clusters there are\n",
    "# Will do a different method in the future though\n",
    "\n",
    "kmeans_clusters = {} # going to be set up in {number_of_clusters : value}\n",
    "for num_clusters in range(1, 100):\n",
    "    cluster_model = sklearn.cluster.KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\")\n",
    "    cluster = cluster_model.fit(embeddings)\n",
    "\n",
    "    kmeans_clusters[num_clusters] = cluster\n",
    "    print(cluster.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fa1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusters[2].labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5151c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
